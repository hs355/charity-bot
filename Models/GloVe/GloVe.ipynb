{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This notebook contains GloVe Embeddings Procedures on Pre-Trained and Custum Corpuses and Tests Performed on GloVe to Determine How it Performs on Text Similarity Tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import itemgetter\n",
    "\n",
    "from scipy import spatial\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.manifold import TSNE\n",
    "from randomwordgenerator import randomwordgenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create Glove Embeddings Dictionaries to Call Word Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1a. GloVe Pre-trained Models in 50 and 300 Dimension Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict_50d = {}\n",
    "with open(\"glove.6B.50d.txt\", 'r', encoding=\"utf-8\") as f: #leave out encoding for older versions\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict_50d[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict_300d = {}\n",
    "with open(\"glove.6B.300d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict_300d[word] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1b. GloVe Custom-Trained Models in 50 and 300 Dimension Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the mission statements and write them to a text file separating each document with new line\n",
    "\n",
    "filename = \"missionStatements.pickle\"\n",
    "\n",
    "with open(filename, \"rb\") as handle:\n",
    "    missionStatements = pickle.load(handle)\n",
    "    \n",
    "for k, v in missionStatements.items():\n",
    "    with open(\"corpus.txt\", \"a\") as f:\n",
    "        text = v[3].replace(\"\\n\", \"\")\n",
    "        f.write(text + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict_tuned = {}\n",
    "with open(\"vectors_50d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict_tuned[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict_tuned_300d = {}\n",
    "with open(\"vectors_300d.txt\", 'r', encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict_tuned_300d[word] = vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Embedding Section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2a. Create embedding for each mission statement for later use in testing: Pre-Trained Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to retrieve embedding vectors\n",
    "def get_Text2Vec(text):\n",
    "    \n",
    "    avgwtext2vec = None\n",
    "    count = 0\n",
    "    \n",
    "    for word in text.split():\n",
    "        \n",
    "        if word in embeddings_dict_300d:\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "            if avgwtext2vec is None:\n",
    "                avgwtext2vec = embeddings_dict_300d[word]\n",
    "            else:\n",
    "                avgwtext2vec = avgwtext2vec + embeddings_dict_300d[word]\n",
    "                \n",
    "    if avgwtext2vec is not None:\n",
    "        avgwtext2vec = avgwtext2vec / count\n",
    "        \n",
    "        return avgwtext2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding for each mission statement and save as dictionary\n",
    "# This is the embedding that combined pretrained model and self corpu\n",
    "\n",
    "text2vec_Embeddings = {}\n",
    "\n",
    "for key, value in missionStatements.items():\n",
    "    \n",
    "    orgID = key # oganization ID\n",
    "    missionStatement = value[3]\n",
    "    \n",
    "    vectors = get_Text2Vec(missionStatement)\n",
    "    text2vec_Embeddings[orgID] = vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings in pickle format for later use\n",
    "with open(\"text2vec_Embeddings.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(text2vec_Embeddings, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2b. Create embedding for each mission statement for later use in testing: Tuned Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to retrieve tuned embedding vectors\n",
    "def get_Text2Vec_Tuned_300d_Combined(text):\n",
    "    \n",
    "    avgwtext2vec = None\n",
    "    count = 0\n",
    "    \n",
    "    for word in text.split():\n",
    "        \n",
    "        if word in embeddings_dict_tuned_300d and word in embeddings_dict_300d:\n",
    "            \n",
    "            count += 1\n",
    "            \n",
    "            if avgwtext2vec is None:\n",
    "                # can test variations\n",
    "                avgwtext2vec = embeddings_dict_tuned_300d[word] + embeddings_dict_300d[word]\n",
    "                #avgwtext2vec = (embeddings_dict_tuned_300d[word] + embeddings_dict_300d[word])/2\n",
    "            else:\n",
    "                # can test variations\n",
    "                avgwtext2vec = avgwtext2vec + (embeddings_dict_tuned_300d[word] + embeddings_dict_300d[word])\n",
    "                #avgwtext2vec = avgwtext2vec + ((embeddings_dict_tuned_300d[word] + embeddings_dict_300d[word])/2)\n",
    "                \n",
    "    if avgwtext2vec is not None:\n",
    "        avgwtext2vec = avgwtext2vec / count\n",
    "        \n",
    "        return avgwtext2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create embedding for each mission statement and save as dictionary\n",
    "# This is the embedding that combined pretrained model and self corpus\n",
    "\n",
    "text2vec_Embeddings_Tuned_300d_Combined = {}\n",
    "\n",
    "for key, value in missionStatements.items():\n",
    "    \n",
    "    orgID = key\n",
    "    missionStatement = value[3]\n",
    "\n",
    "    vectors = get_Text2Vec_Tuned_300d_Combined(missionStatement)\n",
    "    text2vec_Embeddings_Tuned_300d_Combined[orgID] = vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings in pickle format for later use\n",
    "with open(\"text2vec_Embeddings_Tuned_300d_Combined.pickle\", \"wb\") as handle:\n",
    "    pickle.dump(text2vec_Embeddings_Tuned_300d_Combined, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing Section: This section presents several tests. In order to run on different models, substitute in the appropriate dictionaries from Section 2. \n",
    "\n",
    "### Specifically, what the tests do is to take a mission statement from a charity, manipulate the mission statement, e.g. delete parts, add noise, etc., and see if the altered mission statement is successfully matched with the organization's original/unaltered mission statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1. This is just a check to see that everything works. The mission statement is not altered. The unaltered statement is matched with the closest mission statement (it should be itself)  - hundred percent match should be attained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of correct and incorrect matches\n",
    "correct_count = 0\n",
    "incorrect_count = 0\n",
    "\n",
    "# loop through mission statements \n",
    "for k, v in missionStatements.items():\n",
    "\n",
    "    missionStatement = v[3]\n",
    "    orgID = k\n",
    "        \n",
    "    # generate embedding for mission statement\n",
    "    t2v_missionStatement = get_Text2Vec(missionStatement)\n",
    "    collect_Cosine_Values = []\n",
    "\n",
    "    for key, value in text2vec_Embeddings.items():\n",
    "        \n",
    "        cosim = 1 - cosine(value, t2v_missionStatement)\n",
    "        collect_Cosine_Values.append([cosim, key])\n",
    "\n",
    "    # sort to attain highest cosine values\n",
    "    value_list_sorted = sorted(collect_Cosine_Values, key=itemgetter(0))\n",
    "    if value_list_sorted[-1][1] == orgID:\n",
    "        correct_count = correct_count + 1        \n",
    "    else:\n",
    "        incorrect_count = incorrect_count + 1\n",
    "        \n",
    "perc_correct = (correct_count / (correct_count+incorrect_count))*100\n",
    "print(perc_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2. This is the first manipulation. A fraction of the mission statement is selected (either 1/2, 1/5, or 1/10) and then attempted to match with the closest unaltered mission statement. If it is matched with its unaltered version, it's recorded as a correct match. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of correct and incorrect matches\n",
    "correct_count = 0\n",
    "incorrect_count = 0\n",
    "\n",
    "# loop through mission statements \n",
    "for k, v in missionStatements.items():\n",
    "\n",
    "    missionStatement = v[3]\n",
    "    orgID = k\n",
    "    \n",
    "    # vary length of statement e.g. first half, first tenth, second half, etc\n",
    "    missionStatement_length = len(missionStatement)\n",
    "    missionStatement = missionStatement[:int(0.2*missionStatement_length)]\n",
    "        \n",
    "    # generate embedding for mission statement\n",
    "    t2v_missionStatement = get_Text2Vec(missionStatement)\n",
    "    collect_Cosine_Values = []\n",
    "\n",
    "    for key, value in text2vec_Embeddings.items():\n",
    "        \n",
    "        cosim = 1 - cosine(value, t2v_missionStatement)\n",
    "        collect_Cosine_Values.append([cosim, key])\n",
    "\n",
    "    # sort to attain highest cosine values\n",
    "    value_list_sorted = sorted(collect_Cosine_Values, key=itemgetter(0))\n",
    "    if value_list_sorted[-1][1] == orgID:\n",
    "        correct_count = correct_count + 1        \n",
    "    else:\n",
    "        incorrect_count = incorrect_count + 1\n",
    "        \n",
    "perc_correct = (correct_count / (correct_count+incorrect_count))*100\n",
    "print(perc_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3. This test selects a random subset of words from the mission statement and only uses these words in its attempt to match with the correct un-altered mission statement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of correct and incorrect matches\n",
    "correct_count = 0\n",
    "incorrect_count = 0\n",
    "\n",
    "# loop through mission statements \n",
    "for k, v in missionStatements.items():\n",
    "\n",
    "    missionStatement = v[3]\n",
    "    orgID = k\n",
    "    \n",
    "    missionStatementWords_asList = missionStatement.split()\n",
    "    # get number of words in mission statement\n",
    "    no_words = len(missionStatementWords_asList)\n",
    "    half_no_words = int(no_words*0.5) # select to populate with half of the words from original statement\n",
    "\n",
    "    # list to populate with random words from mission statement\n",
    "    select_Words_atRandom = []\n",
    "\n",
    "    # populate new statement until the half mark is reached\n",
    "    while len(select_Words_atRandom) < half_no_words:\n",
    "\n",
    "        randno = random.randint(0, no_words-1)\n",
    "        word = missionStatementWords_asList[randno]\n",
    "\n",
    "        # Non-Unique Version \n",
    "        select_Words_atRandom.append(word)\n",
    "        \n",
    "        # Unique Version: if no duplicates should be in new statement use this version\n",
    "        #if word not in select_Words_atRandom:\n",
    "        #    select_Words_atRandom.append(word)\n",
    "        #else:\n",
    "        #    pass\n",
    "\n",
    "    constructed_missionStatement = \" \".join(select_Words_atRandom)\n",
    "    missionStatement = constructed_missionStatement\n",
    "    \n",
    "    # generate embedding for mission statement\n",
    "    t2v_missionStatement = get_Text2Vec(missionStatement)\n",
    "    collect_Cosine_Values = []\n",
    "\n",
    "    for key, value in text2vec_Embeddings.items():\n",
    "        \n",
    "        cosim = 1 - cosine(value, t2v_missionStatement)\n",
    "        collect_Cosine_Values.append([cosim, key])\n",
    "\n",
    "    # sort to attain highest cosine values\n",
    "    value_list_sorted = sorted(collect_Cosine_Values, key=itemgetter(0))\n",
    "    if value_list_sorted[-1][1] == orgID:\n",
    "        correct_count = correct_count + 1        \n",
    "    else:\n",
    "        incorrect_count = incorrect_count + 1\n",
    "        \n",
    "perc_correct = (correct_count / (correct_count+incorrect_count))*100\n",
    "print(perc_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.1. This test attempts to measure how noise affects the algorithms performance. Fraction of the original mission statement is replaced with random words (noise). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of correct and incorrect matches\n",
    "correct_count = 0\n",
    "incorrect_count = 0\n",
    "\n",
    "# loop through mission statements \n",
    "for k, v in missionStatements.items():\n",
    "\n",
    "    missionStatement = v[3]\n",
    "    orgID = k\n",
    "\n",
    "    missionStatementWords_asList = missionStatement.split()\n",
    "    no_words = len(missionStatementWords_asList)\n",
    "    fraction_of_words = int(no_words * (1/4))\n",
    "\n",
    "    firstPart = missionStatementWords_asList[:fraction_of_words + 1]\n",
    "    # substitute fraction of mission statement with random words to approximate noise\n",
    "    secondPart = randomwordgenerator.generate_random_words(n = int(no_words * (3/4)))\n",
    "\n",
    "    constructed_missionStatement1 = \" \".join(firstPart)\n",
    "    constructed_missionStatement2 = \" \".join(secondPart)\n",
    "    missionStatement = constructed_missionStatement1 + \" \" + constructed_missionStatement2 \n",
    "        \n",
    "    # generate embedding for mission statement\n",
    "    t2v_missionStatement = get_Text2Vec(missionStatement)\n",
    "    collect_Cosine_Values = []\n",
    "\n",
    "    for key, value in text2vec_Embeddings.items():\n",
    "        \n",
    "        cosim = 1 - cosine(value, t2v_missionStatement)\n",
    "        collect_Cosine_Values.append([cosim, key])\n",
    "\n",
    "    # sort to attain highest cosine values\n",
    "    value_list_sorted = sorted(collect_Cosine_Values, key=itemgetter(0))\n",
    "    if value_list_sorted[-1][1] == orgID:\n",
    "        correct_count = correct_count + 1        \n",
    "    else:\n",
    "        incorrect_count = incorrect_count + 1\n",
    "        \n",
    "perc_correct = (correct_count / (correct_count+incorrect_count))*100\n",
    "print(perc_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2.2. This test is the same as in 3.2.1., except that the words are shuffled, so that strings from the original mission statement are not retained.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of correct and incorrect matches\n",
    "correct_count = 0\n",
    "incorrect_count = 0\n",
    "\n",
    "# loop through mission statements \n",
    "for k, v in missionStatements.items():\n",
    "\n",
    "    missionStatement = v[3]\n",
    "    orgID = k\n",
    "    \n",
    "    missionStatementWords_asList = missionStatement.split()\n",
    "    no_words = len(missionStatementWords_asList)\n",
    "    fraction_of_words = int(no_words * (1/4))\n",
    "\n",
    "    firstPart = missionStatementWords_asList[:fraction_of_words + 1]\n",
    "    # substitute fraction of mission statement with random words to approximate noise\n",
    "    secondPart = randomwordgenerator.generate_random_words(n = int(no_words * (3/4)))\n",
    " \n",
    "    # shuffle the selected words\n",
    "    combinedParts = firstPart + secondPart\n",
    "    random.shuffle(combinedParts)\n",
    "    missionStatement = \" \".join(combinedParts)\n",
    "             \n",
    "    # generate embedding for mission statement\n",
    "    t2v_missionStatement = get_Text2Vec(missionStatement)\n",
    "    collect_Cosine_Values = []\n",
    "\n",
    "    for key, value in text2vec_Embeddings.items():\n",
    "        \n",
    "        cosim = 1 - cosine(value, t2v_missionStatement)\n",
    "        collect_Cosine_Values.append([cosim, key])\n",
    "\n",
    "    # sort to attain highest cosine values\n",
    "    value_list_sorted = sorted(collect_Cosine_Values, key=itemgetter(0))\n",
    "    if value_list_sorted[-1][1] == orgID:\n",
    "        correct_count = correct_count + 1        \n",
    "    else:\n",
    "        incorrect_count = incorrect_count + 1\n",
    "        \n",
    "perc_correct = (correct_count / (correct_count+incorrect_count))*100\n",
    "print(perc_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.1. This test replaces the words in the original mission statement with synonyms. This test approximates how the algorithm handles cases where the meaning is retained but the actual words are different. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of correct and incorrect matches\n",
    "correct_count = 0\n",
    "incorrect_count = 0\n",
    "\n",
    "# loop through mission statements \n",
    "for k, v in missionStatements.items():\n",
    "\n",
    "    missionStatement = v[3]\n",
    "    orgID = k\n",
    "    \n",
    "    # procedures to replace words with synonyms start here\n",
    "    missionStatementWords_asList = missionStatement.split()\n",
    "    constructedMissionList = []\n",
    "    \n",
    "    for word in missionStatementWords_asList:\n",
    "        \n",
    "        synonyms = []\n",
    "        \n",
    "        for syn in wordnet.synsets(word):\n",
    "            for l in syn.lemmas():\n",
    "                synonyms.append(l.name())\n",
    "    \n",
    "        a = set(synonyms)\n",
    "        a = list(a)\n",
    "        \n",
    "        #replace with first synonym\n",
    "        try:\n",
    "            constructedMissionList.append(a[0])\n",
    "        except:\n",
    "            constructedMissionList.append(\"\")\n",
    "        \n",
    "    constructed_missionStatement = \" \".join(constructedMissionList)\n",
    "    missionStatement = constructed_missionStatement\n",
    "        \n",
    "    # generate embedding for mission statement\n",
    "    t2v_missionStatement = get_Text2Vec(missionStatement)\n",
    "    collect_Cosine_Values = []\n",
    "\n",
    "    for key, value in text2vec_Embeddings.items():\n",
    "        \n",
    "        cosim = 1 - cosine(value, t2v_missionStatement)\n",
    "        collect_Cosine_Values.append([cosim, key])\n",
    "\n",
    "    # sort to attain highest cosine values\n",
    "    value_list_sorted = sorted(collect_Cosine_Values, key=itemgetter(0))\n",
    "    if value_list_sorted[-1][1] == orgID:\n",
    "        correct_count = correct_count + 1        \n",
    "    else:\n",
    "        incorrect_count = incorrect_count + 1\n",
    "        \n",
    "perc_correct = (correct_count / (correct_count+incorrect_count))*100\n",
    "print(perc_correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3.2. This is the same test as 3.3.1, except that no words that already exist in the mission statement are accepted. That is, if a synonym appears somewhere else in the mission statement, then this will not be considered an acceptable synonym. The procedure instead attempts to replace the word with the second or third synonym. The purpose is to attempt to retain meaning of the mission statement withou retaining any of the word tokens that were present in the original mission statement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep track of correct and incorrect matches\n",
    "correct_count = 0\n",
    "incorrect_count = 0\n",
    "\n",
    "# loop through mission statements \n",
    "for k, v in missionStatements.items():\n",
    "\n",
    "    missionStatement = v[3]\n",
    "    orgID = k\n",
    "    \n",
    "    # procedures to replace words with synonyms start here\n",
    "    missionStatementWords_asList = missionStatement.split()\n",
    "    constructedMissionList = []\n",
    "    \n",
    "    for word in missionStatementWords_asList:\n",
    "        \n",
    "        synonyms = []\n",
    "        \n",
    "        for syn in wordnet.synsets(word):\n",
    "            for l in syn.lemmas():\n",
    "                synonyms.append(l.name())\n",
    "    \n",
    "        a = set(synonyms)\n",
    "        a = list(a)  \n",
    "   \n",
    "        # Attempt to replace the word with the first three suggested synonyms by wordnet\n",
    "        # If the first one equals the word to be replaced or already occurs in mission statement\n",
    "        # then proceed to the second suggestion. If the first three suggestions do not meet the conditions\n",
    "        # or if there are no suggestions (except clause), then leave blank replacement      \n",
    "        \n",
    "        try:\n",
    "            if a[0] != word and a[0] not in missionStatementWords_asList:\n",
    "                constructedMissionList.append(a[0])\n",
    "            elif a[1] != word and a[1] not in missionStatementWords_asList:\n",
    "                constructedMissionList.append(a[1])\n",
    "            elif a[2] != word and a[2] not in missionStatementWords_asList:\n",
    "                constructedMissionList.append(a[2])\n",
    "            else:\n",
    "                #pass\n",
    "                constructedMissionList.append(\"\")\n",
    "        except:\n",
    "            constructedMissionList.append(\"\")\n",
    "        \n",
    "    constructed_missionStatement = \" \".join(constructedMissionList)\n",
    "    missionStatement = constructed_missionStatement    \n",
    "    \n",
    "    # generate embedding for mission statement\n",
    "    t2v_missionStatement = get_Text2Vec(missionStatement)\n",
    "    collect_Cosine_Values = []\n",
    "\n",
    "    for key, value in text2vec_Embeddings.items():\n",
    "        \n",
    "        cosim = 1 - cosine(value, t2v_missionStatement)\n",
    "        collect_Cosine_Values.append([cosim, key])\n",
    "\n",
    "    # sort to attain highest cosine values\n",
    "    value_list_sorted = sorted(collect_Cosine_Values, key=itemgetter(0))\n",
    "    if value_list_sorted[-1][1] == orgID:\n",
    "        correct_count = correct_count + 1        \n",
    "    else:\n",
    "        incorrect_count = incorrect_count + 1\n",
    "        \n",
    "perc_correct = (correct_count / (correct_count+incorrect_count))*100\n",
    "print(perc_correct)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
